---
title: 数据分析中的常用模型与方法
hidden: false
katex: false
date: 2022-08-12 10:25:05
id: Models-and-Methods-in-Data-Analysis
sticky:
categories:
- 学习笔记
tags:
- Python
- 数学建模
- 数据分析
---

本文将介绍一些常用的数据分析方法与模型以及其 Python 实现。

<!-- more -->

## 数据预处理

### 异常值的筛选与处理

异常值（outlier）是指与其他观测值有显著差异的数据点，异常值可能会导致统计分析中出现严重的问题，因此需要对其进行处理。能妥善处理异常值的估计量，称为「稳健」。

#### 异常值的检测

##### 基于正态分布（Z-Score）的一元异常值检测

若数据为一维，在数据服从正态分布的假设下，以参数估计近似分布的均值与标准差，利用 $3-\sigma$ 原则检测异常值（即 z-score 的绝对值超过 3）。

##### 基于数字异常值（Numeric Out）的一元异常值检测

若数据未通过正态性检验，则可使用数字异常值方法来进行异常值的检验。计算数据的第一和第三四分位数，分别记为 $Q_1, Q_2$，则可确定非异常数据的上下界为

$$\begin{aligned} &\text{sup} = Q_1 - k(IQR) \\ &\text{inf} = Q_3 + k(IQR) \end{aligned} $$

其中 $IQR = Q_3 - Q_1$，且 $k \geq 0$，一般取 $k = 1.5$，即上下晶须的箱线图。

##### 基于正态分布的 n 维数据异常值检验

而若数据为 n 维，若某些维度的数据服从正态分布，则可对这些维度的数据进行单独的正态分布异常值检测。若不服从正态分布，则可使用 Z-Score 进行检测。

而若这 n 维数据服从于 n 元正态分布，则可计算 n 维均值向量

$$ \vec\mu = (E(x_1), E(x_2), \cdots, E(x_n)) $$

与协方差矩阵

$$ \Sigma = [\text{Cov}(x_i, x_j), i, j \in {1, 2, \cdots, n}] $$

对于某个数据 $\vec x$，计算其概率

$$ p(\vec x) = \dfrac 1 {(2\pi)^{n/2}|\Sigma|^{1/2}}\text{exp}\left[-\dfrac12(\vec x - \vec\mu)'\Sigma^{-1}(\vec x - \vec\mu)\right] $$

概率在 $3-\sigma$  外的即为异常值。

##### 孤立森林（Isolation Forest）算法

使用孤立森林算法的数据需要满足以下两点：

1. 异常数据占比很小；
2. 异常点的特征（如坐标）与正常点的差异很大。

通常首先作图对数据的分布进行直观的观测，**若在某一（些）小区域中数据点十分集中**，而剩余区域中数据点**很少且不成群**，则可使用孤立森林算法。

该算法的思想为使用一个随机的超平面对数据空间进行切割，生成两个子空间，然后继续随机选取超平面进行对生成的两个子空间进行切割……不断进行这个过程直到每个子空间中只包含一个数据点。在这个过程中，密度很高的簇需要许多次切割才会停止，而分布稀疏的点很快就会停止在一个子空间中。

**训练过程分为三步，如下：**

1. 单棵树的训练

   1. 在训练数据中随机选择 $\Psi$ 个样本点作为子样本，放入一棵孤立树的根节点；
   2. 随机指定一个维度，在当前节点的数据范围内，随机产生一个切割点 p —— 切割点产生于当前节点数据中指定维度的最大值与最小值之间；
   3. 此切割点的选取生成了一个超平面，将当前节点数据空间切分为 2 个子空间：将当前所选唯独下小于 p 的点放在当前节点的左分支，把大于等于 p 的点放在当前节点的右分支；
   4. 在节点的左右分支节点分别递归比上步骤 2 与 3，不断构造新的叶子节点，直到叶子节点上只有一个数据（无法再继续切割）或树已经生长到了所设定的最大高度；

2. 整合全部孤立树的结果

   1. 由于切割过程是完全随机的，所以需要用集成学习（Ensemble Learning）的方法来使结果收敛，即反复从头开始切分，然后计算每次切分结果的平均值。

   2. 获得 t 个孤立树后，单棵树的训练就结束了，然后需要进行评估测试数据，即计算异常分数 $s$，对于每个样本 $x$，通过公式

       $$ s(x, \psi) = 2^{-\frac{E(h(x))}{c(\psi)}} $$ 

      计算异常得分，其中 $h(x)$ 为 $x$ 在每棵树的高度，$c(\psi)$ 为给定样本数 $\varPsi$ 时路径长度的平均值，用来对样本 $x$ 的路径长度 $h(x)$ 进行标准化处理。一般我们设置树的棵数为 100 即可；

3. 异常得分分析

   1. 若异常得分接近 1，则该点一定是异常点；
   2. 若异常得分远小于 0.5，则该点一定不是异常点；
   3. 若所有点的异常得分都在 0.5 左右，则样本中很可能不存在异常点。

   **需要注意的点：**

   1. 若样本中异常比例过高，则结果可能不理想。因此该算法仅适用于异常比例较低的样本；
   2. 异常检测与具体应用场景密切相关，因此需要过滤不相关的特征。

   **该算法的优点**：

   1. 在训练过程中，每棵孤立树都是随机选取部分样本，保证了算法的精确性与稳定性；
   2. 不同于 KMeans、DBSCAN 等算法，孤立森林不需要计算有关距离、密度的指标，可大幅度提升速度，减小系统开销；
   3. 由于基于集成学习（Ensemble Learning），所以有线性时间复杂度。通常树的数量越多，算法越稳定。

   **算法实现**：

   我们使用 Sklearn 中的 IsolationForest 实现该算法

   ```python
   from sklearn.ensemble import IsolationForest
   clf = IsolationForest()
   ```

   其中常用参数设置如下：

   - `n_estimators: int, (optional, default=100)`：指定该森林中生成的随机树数量；
   - `max_samples: int, float, 'auto', (optional, default='auto')`：用来训练随机数的样本数量，即子采样的大小，若为 int 类型则设置子采样大小为该数；若为 float 类型则取总数的该比例的样本；若为 `'auto'`，则设置为 256 和样本总数中最小的那一个；
   - `contamination: float, 'auto', (optional, default='auto')`：异常数据占给定的数据集的比例，为 `'auto'`时为 0.22；
   - `max_features: int, float, (optional, default=1)`：最大特征数，指定从总样本 X 中抽取来训练每棵树的属性的数量；
   - `bootstrap: bool, (optional, default=False)`：构建 Tree 时，下次是否替换采样，为 True 为替换，则各个树可放回地对训练数据进行采样；为 False 为不替换，即执行不放回的采样；
   - `n_jobs: int, None, (optional, default=None)`：在运行 fit () 和 predict () 函数时并行运行的作业数量。除了在 joblib.parallel_backend 上下文的情况下，None 表示为 1，设置为 -1 则表示使用所有可以使用的处理器；
   - `random_state: int, RandomState instance, None, (optional, default=None)`：如果设置为 int 常数，则该 random_state 参数值是用于随机数生成器的种子；如果设置为 RandomState 实例，则该 random_state 就是一个随机数生成器；如果设置为 None，则该随机数生成器就是使用在 np.random 中 RandomState 实例；
   - `verbose: int, (optional, default=0)`：控制树构建过程的冗长性；
   - `warm_start: bool, (optional, default=False)`：当设置为 `True` 时，重用上一次调用的结果去 fit，添加更多的树到上一次的森林 1 集合中；否则就 fit 一整个新的森林；

   其中常用属性如下：

   - `estimators_`：构造好的子树的集合；
   - `estimators_samples`：每个子树抽取的样本的子集；
   - `max_samples_`：样本的真正数量；

   常用方法介绍：

   - `fit(X)`：对模型进行训练；
   - `fit_predict(X)`：对模型进行训练并评价；
   - `decision_function(X)`：计算异常得分。

##### DBSCAN 密度聚类算法

当某一数据集存在多个密度核心时，可使用 DBSCAN 聚类进行异常值的筛选

## 参考资料

[1] [有哪些比较好的做异常值检测的方法？ - 阿里云云栖号的回答 - 知乎](https://www.zhihu.com/question/38066650/answer/549125707)
[2] [有哪些比较好的做异常值检测的方法？ - 张戎的回答 - 知乎](https://www.zhihu.com/question/38066650/answer/107801822)
[3] [异常检测算法 -- 孤立森林（Isolation Forest）剖析](https://zhuanlan.zhihu.com/p/74508141)
[4] [【机器学习】密度聚类 DBSCAN 与异常检测](https://blog.csdn.net/weixin_31866177/article/details/89416513)
